% !Mode:: "TeX:UTF-8"
%# -*- coding:utf-8 -*-

%% 南京大学学位论文的示例文档
%% 作者：njuhan: https://github.com/njuHan
%% 源模版repo: https://github.com/njuHan/njuthesis-nju-thesis-template

\documentclass[winfonts,master,twoside]{njuthesis}
%% njuthesis 文档类的可选参数有：
%%   winfonts, linuxfonts, macfonts, adobefonts winfonts 选项使得文档使用Windows 系统提供的字体；linuxfonts 选项使得文档使用Linux 系统提供的字体；macfonts 选项使得文档使用Mac 系统提供的字体；adobefonts 选项使得文档使用Adobe提供的OTF中文字体(需自行下载安转)
%%   phd/master/bachelor 选择博士/硕士/学士论文
%%   twoside 或 oneside 指定排版的文档为双面打印或单面打印格式（twoside会使得chapter 章节从奇数页开始，即纸张的正面开始，因此会出现一些空白的页面）
%%   nobackinfo 取消封二页导师签名信息。注意，按照南大的规定，是需要签名页的。



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% set up labelformat and labelsep for subfigure 详见： http://www.latexstudio.net/archives/8652.html
\captionsetup[subfigure]{labelformat=simple, labelsep=space}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置《国家图书馆封面》的内容，仅博士论文才需要填写

% 设置论文按照《中国图书资料分类法》的分类编号
\classification{0175.2}
% 设置论文按照《国际十进分类法UDC》的分类编号
% 该编号可在下述网址查询：http://www.udcc.org/udcsummary/php/index.php?lang=chi
\udc{004.72}
% 国家图书馆封面上的论文标题第一行，不可换行。此属性可选，默认值为通过\title设置的标题。
\nlctitlea{论文标题第一行}
% 国家图书馆封面上的论文标题第二行，不可换行。此属性可选，默认值为空白。
\nlctitleb{论文标题第二行}
% 国家图书馆封面上的论文标题第三行，不可换行。此属性可选，默认值为空白。
\nlctitlec{}
% 导师的单位名称及地址
\supervisorinfo{南京大学计算机科学与技术系~~南京市汉口路22号~~210093}
% 答辩委员会主席
\chairman{张三丰~~教授}
% 第一位评阅人
\reviewera{阳顶天~~教授}
% 第二位评阅人
\reviewerb{张无忌~~副教授}
% 第三位评阅人
\reviewerc{黄裳~~教授}
% 第四位评阅人
\reviewerd{郭靖~~研究员}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的中文封面

% 单行论文标题，不可换行
\title{南京大学毕业论文\LaTeX 模板}

% 如果论文标题过长，可以分两行，第一行用\titlea{}定义，第二行用\titleb{}定义，
% 使用以下3行：
%\title{} %用于覆盖单行标题内容为空
%\titlea{长标题第一行}  %第一行标题写这里
%\titleb{长标题第二行用于长标题换行} %第二行标题写这里
% 注意： \title 不能都注释，它用于控制标题选择双行还是单行。\title{}如果内容为空，则编译\titlea{},titleb{}双行标题，否则编译单行标题


% 论文作者姓名
\author{陈晨曦}
% 论文作者联系电话
\telphone{xxxx}
% 论文作者电子邮件地址
\email{sample@smail.nju.edu.cn}
% 论文作者学生证号
\studentnum{MF1933004}
% 论文作者入学年份（年级）
\grade{2019}
% 论文作者毕业年份（届）, 出版授权书的学位年度
\graduateyear{2022}
% 导师姓名职称
\supervisor{叶保留教授}
% 导师的联系电话
\supervisortelphone{}
% 论文作者的学科与专业方向
\major{计算机技术}
% 论文作者的研究方向
\researchfield{联邦学习}
% 论文作者所在院系的中文名称
\department{计算机科学与技术系}
% 论文作者所在学校或机构的名称。此属性可选，默认值为``南京大学''。
\institute{南京大学}
% 论文的提交日期，需设置年、月、日。
\submitdate{xxxx年 xx 月 xx 日}
% 论文的答辩日期，需设置年、月、日。
\defenddate{xxxx年 xx 月 xx 日}
% 论文的定稿日期，需设置年、月、日。
% 此属性可选，若注释\date{}，则默认值为最后一次编译时的日期，精确到日。
% \date{2019年5月20日}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的英文封面

% 论文的英文标题，不可换行
\englishtitle{\LaTeX \;  NJU thesis template }
% 论文作者姓名的拼音
\englishauthor{Chenxi Chen}
% 导师姓名职称的英文
\englishsupervisor{ Professor Baoliu Ye}
% 论文作者学科与专业的英文名
\englishmajor{Computer Science and Technology}
% 论文作者所在院系的英文名称
\englishdepartment{Department of Computer Science and Technology}
% 论文作者所在学校或机构的英文名称。此属性可选，默认值为``Nanjing University''。
\englishinstitute{Nanjing University}
% 论文完成日期的英文形式，它将出现在英文封面下方。需设置年、月、日。日期格式使用美国的日期
% 格式，即``Month day, year''，其中``Month''为月份的英文名全称，首字母大写；``day''为
% 该月中日期的阿拉伯数字表示；``year''为年份的四位阿拉伯数字表示。
% 此属性可选，若注释掉\englishdate{}，则默认值为最后一次编译时的日期。
% \englishdate{May 20, 2019}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的中文摘要

% 设置中文摘要页面的论文标题及副标题的第一行。
% 此属性可选，其默认值为使用|\title|命令所设置的论文标题
\abstracttitlea{标题第一行}
% 设置中文摘要页面的论文标题及副标题的第二行。
% 此属性可选，其默认值为空白
\abstracttitleb{标题第二行用于长标题换行}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 设置论文的英文摘要

% 设置英文摘要页面的论文标题及副标题的第一行。
% 此属性可选，其默认值为使用|\englishtitle|命令所设置的论文标题
\englishabstracttitlea{englishabstracttitlea}
% 设置英文摘要页面的论文标题及副标题的第二行。
% 此属性可选，其默认值为空白
\englishabstracttitleb{nglishabstracttitleb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 盲审命令，空白字段设置请看 .cls文件 \newcommand*{\blind}
%% 此外，请按照盲审要求自行去掉个人简历、致谢等页面中的个人信息
%\blind

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 制作国家图书馆封面（博士学位论文才需要）
%\makenlctitle
% 制作中文封面
\maketitle
% 制作英文封面
\makeenglishtitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 开始前言部分
\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的中文摘要
\begin{abstract}
\lipsum[1-2]

%通过改变链路中子流的个数，分配不同的数据流量给不同的链路。

% 中文关键词。关键词之间用中文全角分号隔开，末尾无标点符号。
\keywords{关键词1 \quad 关键词2 }
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的英文摘要
\begin{englishabstract}
%\lipsum[2]
\textcolor{red}{As a new machine learning paradigm, federated learning uses distributed edge devices to train the model without exposing privacy data, providing thorough protection for user privacy. However, federated learning faces severe communication challenges due to frequent and massive model parameter updates. Moreover, its training delay suffers from the heterogeneity of edge devices. 
Federated learning places the majority of the deep learning (DL) training tasks on the edge devices with heterogeneous computation and communication abilities, leading to tremendous training delay. And the intense exchanges of heavy model parameters severely exacerbate this problem. DL models' layered structure allows a device to partition and offload part of the model training to the server, which reduces the size of transmitted data and computation amount of devices. This paper proposes a method based on the model partition, which speeds up training by reducing the high communication overhead and mitigating the heterogeneous problem of federated learning in the edge environment. We model the training process of a partitioned model in the wireless network, construct an optimization problem that minimizes the training through adjustment of partition and resource allocation strategies. And we design an polynomial algorithm that finds a sub-optimal to the problem by dividing it into two sub-problems and solving them using polynomial algorithms. Extensive simulation results verify that our algorithm can effectively reduce the training delay compared to existing solutions.}
%Rate adaptation can be implemented by adjusting the number of subflows on each path.

% 英文关键词。关键词之间用英文半角逗号隔开，末尾无符号。
\englishkeywords{federated learning\quad model partition}
\end{englishabstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 论文的前言，应放在目录之前，中英文摘要之后
%
\begin{preface}
\lipsum[1]
\vspace{1cm}
\begin{flushright}
作者\\
20xx年夏于南京大学
\end{flushright}

\end{preface}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成论文目录
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成插图清单。如无需插图清单则可注释掉下述语句。
\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成附表清单。如无需附表清单则可注释掉下述语句。
\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 开始正文部分
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 学位论文的正文应以《绪论》作为第一章
\chapter{绪论}\label{chapter_introduction}
\section{研究背景}
%使用.bib文件管理参考文献引用，引用示例：\cite{BHR12}.\par
%\lipsum[1]
基于神经网路的深度学习在人工智能的各种领域，如图像识别、语音识别和自然语言处理，中取得了前所未有的精度。深度学习技术也因此被广泛应用于现实生活场景，从根本上改变了人们的生活方式。主流的深度学习模型训练框架都是基于云的，这类框架需要首先将用户产生的海量原始数据收集到云中心，随后集群使用收集来的数据训练深度学习模型。然而，这样的训练方式暴露了一些缺点。一是隐私问题。基于云的训练矿建允许服务器直接访问用户的数据，这威胁到用户的隐私安全。随着网络的普及，越来越多的人意识到保护网络隐私的重要性，关于网络隐私保护的立法也在不断完善，例如：欧盟通用数据保护条例 (GDPR)\cite{GDPR}、《中华人民共和国网络安全法》\cite{cybersecurity}、《中华人民共和国民法通则》\ cite{generalprinciple} 限制了对用户敏感数据的使用。

其次，随着物联网的发展，越来越多的联网设备——智能手机、摄像机和智能传感器等被生产和使用，人们平均每天产生的数据也在急剧增长。根据思科的2020年度互联网报告\cite{ciscoannual}，网络设备数量将从 2018 年的 184 亿台增加到 2023 年的 293 亿台。存储如此规模的数据势必会给云中心带来巨大的存储压力。

\textcolor{red}{为了解决上述问题，一种新的深度学习训练框架——联邦学习\cite{DBLP:conf/aistats/McMahanMRHA17} 被提出。在联邦学习中，参与者（参与训练的最终用户）使用他们的数据合作训练联邦学习服务器所需的深度学习模型，然后将他们的模型更新而不是原始数据上传到服务器进行聚合。这些步骤重复多次，直到达到所需的精度。这种训练方式可以防止服务器直接访问用户的敏感数据；因此，用户隐私受到保护。并且服务器不需要存储所有的训练数据，可以节省相当多的存储空间。此外，由于计算能力以惊人的速度增长的无数终端设备的杠杆作用，联邦学习在大规模并行计算中具有巨大的潜力。}

\textcolor{red}{尽管联邦学习相对于基于云的训练方法具有优势，但在实践中的部署方面面临着一些挑战。主要挑战之一是通信开销。在联邦学习中训练 DL 模型涉及数百甚至数千轮模型交换，而如今的 DL 模型往往具有巨大的参数大小——数百兆字节或更多。因此，使用联邦学习来训练 DL 模型需要大量的数据传输。计算设备的异构性带来了另一个挑战。联邦学习采用批量同步并行（BSP）聚合，即在每一轮训练中，服务器在所有设备完成计算和上传结果后聚合更新。因为联邦学习中设备的计算和通信能力有很大差异，但是这些设备的计算量和通信量是相同的。所以在每一轮中，完成任务的最快设备必须等待最慢的设备。上述两个挑战都严重减缓了联邦学习的训练。}

\textcolor{red}{这些年来已经提出了应对这些挑战的研究。处理高通信开销的主流方法是压缩\cite{DBLP:journals/corr/KonecnyMYRSB16}\cite{DBLP:conf/iclr/LinHM0D18}\cite{DBLP:conf/icml/TangYLZL19}\cite{DBLP:conf/ nips/VogelsKJ19}\cite{DBLP:journals/corr/IandolaMAHDK16}\cite{DBLP:conf/iclr/PolinoPA18}，它使用量化、稀疏化或修剪来减小模型或更新的大小。然而，由于压缩过程中的信息丢失，这种压缩方法会降低模型的精度。通常，模型在压缩后需要进行更多轮训练才能达到与压缩前模型相同的精度，这最终会增加整体训练时间。异步并行 (ASP) 聚合\cite{DBLP:conf/hotos/CiparHKLGGKX13}，其中服务器会在每次更新时立即聚合，以及过时的同步并行 (SSP)\cite{DBLP:conf/cloud/HarlapCDWGGGX16}， BSP 和 ASP 的折衷方案，都可以用来解决异构问题。然而，使用 ASP 或 SSP 模型收敛会恶化，这会导致收敛速度慢和模型精度低。此外，压缩和 ASP 聚合都无法应对上述挑战。}
\section{章节}\label{subsec:mptcp_conges}
\lipsum[1]

\section{章节}
\lipsum[1]


\section{论文结构}
\lipsum[1]
\begin{itemize}
\item 一级item
 \begin{itemize}
 \item 二级item
	\begin{itemize}
	\item 三级item

	\end{itemize}

 \end{itemize}
\item 一级item

\end{itemize}


\chapter{算法}

\begin{algorithm}[htbp]
  \caption{算法名字}
  \label{alg:alg1}
  \begin{algorithmic}[1]
        \REQUIRE 这是输入
        \ENSURE 这是输出
        \WHILE {flag}
		      \STATE 这是语句
        \ENDWHILE
  \end{algorithmic}
\end{algorithm}

\chapter{实验验证}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{./figure/github.jpg}
  \caption{单图示例}
  \label{fig:system}
\end{figure}

实验硬件设备如图\ref{img:1}所示。
\begin{figure}[htbp]
\begin{minipage}[t]{0.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./figure/github.jpg}
\caption{实验硬件设备总览}
\label{img:1}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./figure/github.jpg}
\caption{实验测量示意图}
\label{img:2}
\end{minipage}
\end{figure}

图\ref{fig:sub}所示子图\ref{subfig:a}和子图\ref{subfig:b}。
\begin{figure}[H]
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{./figure/github.jpg}
		\caption{子图}
		\label{subfig:a}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{./figure/github.jpg}
		\caption{子图}
		\label{subfig:b}
	\end{subfigure}
\caption{子图样例}
\label{fig:sub}
\end{figure}


\chapter{总结与展望}
\lipsum[1]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 致谢，应放在《结论》之后好的
\begin{acknowledgement}
%thanks
\lipsum[1]

\end{acknowledgement}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% 参考文献。应放在\backmatter之前。
% 推荐使用BibTeX，若不使用BibTeX时注释掉下面一句。
%\nocite{*}
\bibliography{sample}


% 附录，必须放在参考文献后，backmatter前
\appendix
\chapter{附录代码}\label{app:1}
\section{main函数}
\begin{lstlisting}[language=C]
int main()
{
	return 0;
}
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 书籍附件
\backmatter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 作者简历与科研成果页，应放在backmatter之后
\begin{resume}
% 论文作者身份简介，一句话即可。
\begin{authorinfo}
\noindent 韦小宝，男，汉族，1985年11月出生，江苏省扬州人。
\end{authorinfo}
% 论文作者教育经历列表，按日期从近到远排列，不包括将要申请的学位。
\begin{education}
\item[2007年9月 --- 2010年6月] 南京大学计算机科学与技术系 \hfill 硕士
\item[2003年9月 --- 2007年6月] 南京大学计算机科学与技术系 \hfill 本科
\end{education}
% 论文作者在攻读学位期间所发表的文章的列表，按发表日期从近到远排列。
\begin{publications}
\item Xiaobao Wei, Jinnan Chen, ``Voting-on-Grid Clustering for Secure
  Localization in Wireless Sensor Networks,'' in \textsl{Proc. IEEE International
    Conference on Communications (ICC) 2010}, May. 2010.
\item Xiaobao Wei, Shiba Mao, Jinnan Chen, ``Protecting Source Location Privacy
  in Wireless Sensor Networks with Data Aggregation,'' in \textsl{Proc. 6th
    International Conference on Ubiquitous Intelligence and Computing (UIC)
    2009}, Oct. 2009.
\end{publications}
% 论文作者在攻读学位期间参与的科研课题的列表，按照日期从近到远排列。
\begin{projects}
\item 国家自然科学基金面上项目``问题研究''
（课题年限~2010年1月 --- 2012年12月），负责相关问题的研究。
\end{projects}
\end{resume}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 生成《学位论文出版授权书》页面，应放在最后一页
\makelicense

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
